{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jEfz1So1Z-N"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/leonardocrociani/MCTS-Pokemon-Battle-Policy\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "# **MCTS Pok√©mon VGC Competition**  \n",
        "\n",
        "_Course: Artificial Intelligence Fundamentals, A.Y. 2024/25_  \n",
        "_Author: Leonardo Crociani_  \n",
        "_Github Repository: [MCTS-Pokemon-Battle-Policy](https://github.com/leonardocrociani/MCTS-Pokemon-Battle-Policy)_\n",
        "\n",
        "<small>**Important Notice:** The framework's creator announced on the official Discord channel that a new engine version is expected in the third week of January. If this update includes API changes, some code discrepancies may arise. This code was developed using version **3.0.5.7** of the master branch.</small>\n",
        "\n",
        "[Are you searching the text-only version?](https://colab.research.google.com/github/leonardocrociani/MCTS-Pokemon-Battle-Policy/blob/main/AIF_only_text.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Introduction**  \n",
        "\n",
        "For the Artificial Intelligence Fundamentals exam, I chose to work on the **Pok√©mon VGC AI competition project**, specifically pursuing the **Battle Policy track**.  \n",
        "\n",
        "The implemented battle policy interacts with a predefined environment‚Äîa highly convenient framework [1]‚Äîthat manages the game state and its updates, allowing the user to focus solely on policy development.  \n",
        "\n",
        "**The goal of the policy is to defeat any other battle policy, regardless of the team composition used.**  \n",
        "\n",
        "The game of Pok√©mon is particularly interesting. When considering teams of three Pok√©mon, the branching factor for each player's possible states is at most **6** (four attack moves and two switches). Both players select their moves simultaneously, and the new game state is then evaluated.\n",
        "\n",
        "There are multiple factors to take into account:\n",
        "\n",
        "1. An agent may encounter issues such as infinite looping, continuously selecting the `switch` action.  \n",
        "\n",
        "2. In the short term (i.e., looking one turn ahead), we can make a reasonable estimation of the new state, but in the long term, this estimation loses significance due to several factors:  \n",
        "  a. The game incorporates **probability**, introducing randomness into outcomes.  \n",
        "  b. During the initial phases, the game is **not fully observable**, meaning we might complete a battle without ever having full knowledge of the opponent‚Äôs team.  \n",
        "\n",
        "For these reasons, I decided to implement a **Monte Carlo Tree Search based Battle Policy**. My goal was to leverage the robustness of MCTS, complemented by heuristics to guide the playouts and the node selection effectively.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVPv11YuxsF8"
      },
      "outputs": [],
      "source": [
        "!git clone https://gitlab.com/DracoStriker/pokemon-vgc-engine.git\n",
        "!pushd pokemon-vgc-engine && git reset --hard 2f23335a8f6796d46b4aabfb163c0f17c0dc5fe6 && popd\n",
        "!pip install ./pokemon-vgc-engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v2ejJDA4x6C1"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from copy import deepcopy\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "from vgc.datatypes.Objects import GameState, PkmStatus, Pkm, PkmTeam, PkmType, WeatherCondition, PkmMove, PkmStat\n",
        "from vgc.datatypes.Constants import DEFAULT_N_ACTIONS, TYPE_CHART_MULTIPLIER\n",
        "from vgc.behaviour import BattlePolicy, TeamSelectionPolicy, TeamBuildPolicy\n",
        "from vgc.competition.Competitor import Competitor, CompetitorManager\n",
        "from vgc.util.generator.PkmRosterGenerators import RandomPkmRosterGenerator\n",
        "from vgc.util.generator.PkmTeamGenerators import RandomTeamFromRoster\n",
        "from vgc.behaviour.TeamSelectionPolicies import FirstEditionTeamSelectionPolicy\n",
        "from vgc.behaviour.TeamBuildPolicies import RandomTeamBuilder\n",
        "from vgc.competition.BattleMatch import BattleMatch\n",
        "\n",
        "# competitor battle policy\n",
        "from vgc.behaviour.BattlePolicies import Minimax, TypeSelector, TunedTreeTraversal, PrunedBFS\n",
        "\n",
        "\n",
        "SWITCH_THRESHOLD = 3.75\n",
        "DAMAGE_THRESHOLD = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jNMlXUhByLp7"
      },
      "outputs": [],
      "source": [
        "# utility class\n",
        "class CompetitorWrapper(Competitor):\n",
        "  def __init__(self, name: str = 'Competitor'):\n",
        "    self._name = name\n",
        "    self._team_selection_policy = FirstEditionTeamSelectionPolicy()\n",
        "    self._team_build_policy = RandomTeamBuilder()\n",
        "\n",
        "  @property\n",
        "  def name(self):\n",
        "    return self._name\n",
        "\n",
        "  @property\n",
        "  def team_build_policy(self) -> TeamBuildPolicy:\n",
        "    return self._team_build_policy\n",
        "\n",
        "  @property\n",
        "  def team_selection_policy(self) -> TeamSelectionPolicy:\n",
        "    return self._team_selection_policy\n",
        "\n",
        "  @property\n",
        "  def battle_policy(self) -> BattlePolicy:\n",
        "    return self._battle_policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRQnSMnfP2pQ"
      },
      "source": [
        "## **Related Work**  \n",
        "\n",
        "The application of artificial intelligence in Pok√©mon Video Game Championships has been explored through various methodologies, notably Monte Carlo Tree Search and Reinforcement Learning.  \n",
        "\n",
        "MCTS has been successfully utilized in developing AI agents for Pok√©mon battles. **Norstr√∂m (2019)** identified MCTS as an effective algorithm for full-scale Pok√©mon battles, emphasizing its balance between exploration and exploitation in decision-making processes [2].  \n",
        "\n",
        "Additionally, the implementation of Information Set Monte Carlo Tree Search has been proposed by **Ihara H., et al. (2018)** [3] to address the imperfect information scenarios inherent in Pok√©mon battles, aiming to mitigate strategy fusion issues caused by determinization.  \n",
        "\n",
        "RL approaches have also been applied to develop competitive Pok√©mon battle strategies. **Kalose and Kaya (2018)** [4] explored RL techniques to determine optimal battle strategies, demonstrating RL's potential in handling the complexities of Pok√©mon battles.  \n",
        "\n",
        "Moreover, the integration of deep reinforcement learning and neuroevolution has been investigated by **Rodriguez, G., et al. (2024)** [5] to enhance AI performance in the VGC context, highlighting the adaptability of these methods to the game's dynamic environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N42-960NBgml"
      },
      "source": [
        "## **Methodologies**  \n",
        "\n",
        "The Monte Carlo Tree Search algorithm runs an arbitrary number of simulations from the current game state and selects the **most promising** action.  \n",
        "\n",
        "The algorithm consists of four phases:  \n",
        "- *Selection*: Identifies the most suitable node to expand within the current tree of states.  \n",
        "- *Expansion*: Expands the selected node by adding new child nodes.  \n",
        "- *Simulation*: Simulates gameplay from the newly expanded node down to a terminal state.  \n",
        "- *Backpropagation*: Propagates the simulation results back up the tree to update node values.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "olSyZHB1y2dT"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, parent, connection_action):\n",
        "        self.game_state: GameState = None\n",
        "        self.parent = parent\n",
        "        self.connection_action = connection_action\n",
        "        self.visits = 0\n",
        "        self.utility = 0.0\n",
        "        self.children = []\n",
        "\n",
        "    def is_fully_expanded(self):\n",
        "        valid_action_count = sum(1 for i in range(DEFAULT_N_ACTIONS)\n",
        "                               if not is_action_suboptimal(self.game_state, i))\n",
        "        return len(self.children) == valid_action_count\n",
        "\n",
        "    def best_child(self):\n",
        "        return max(self.children, key=lambda child: child.utility / (child.visits + 1e-6), default=None)\n",
        "\n",
        "class MCTSPolicy(BattlePolicy):\n",
        "    def __init__(self, max_iterations: int = 10_000, C:int = 10, debug = True, seed: int = 1):\n",
        "        self.max_iterations = max_iterations\n",
        "        self.debug = debug\n",
        "        self.turns = 0\n",
        "        self.max_C = C*3\n",
        "        self.C = C\n",
        "        self.min_C = C\n",
        "        random.seed(seed)\n",
        "\n",
        "    def get_action(self, game_state: GameState):\n",
        "        self.turns += 1\n",
        "\n",
        "        self.C = max(self.min_C, self.max_C/(self.turns**1.2))\n",
        "\n",
        "        print(f'Turn={self.turns}, C={self.C}')\n",
        "\n",
        "        root = Node(None, None)\n",
        "        root.game_state = deepcopy(game_state)\n",
        "\n",
        "        action = self.mcts_search(root)\n",
        "        return action\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        return state.teams[1].active.hp == 0 or state.teams[0].active.hp == 0\n",
        "\n",
        "    def is_switch(self, action):\n",
        "        return action >= 4\n",
        "\n",
        "    def playout(self, node):\n",
        "        state = deepcopy(node.game_state)\n",
        "        cycles = 0\n",
        "        num_switches = 0\n",
        "        while not self.is_terminal(state):\n",
        "            matchup_scores = evaluate_team_matchups(state)\n",
        "            current_score = matchup_scores[0]\n",
        "            best_alternative = max(matchup_scores[1:], default=float('-inf'))\n",
        "\n",
        "            valid_actions = []\n",
        "\n",
        "            # if we have a significantly better matchup available, prioritize switching\n",
        "            if (best_alternative - SWITCH_THRESHOLD) > current_score:\n",
        "                # Find switch actions that improve our position\n",
        "                for i in range(len(state.teams[0].active.moves), DEFAULT_N_ACTIONS):\n",
        "                    switch_index = i - len(state.teams[0].active.moves)\n",
        "                    if switch_index < len(matchup_scores) - 1:\n",
        "                        if matchup_scores[switch_index + 1] > current_score:\n",
        "                            valid_actions.append(i)\n",
        "\n",
        "            # if no good switches found, consider moves that aren't suboptimal\n",
        "            if not valid_actions:\n",
        "                valid_actions = [i for i in range(DEFAULT_N_ACTIONS)\n",
        "                            if not is_action_suboptimal(state, i)]\n",
        "\n",
        "            # if still no valid actions, allow all actions\n",
        "            if not valid_actions:\n",
        "                valid_actions = list(range(DEFAULT_N_ACTIONS))\n",
        "\n",
        "            my_action = random.choice(valid_actions)                # i choose random i a list of optimized valid actions\n",
        "            opp_action = random.randint(0, DEFAULT_N_ACTIONS - 1)   # opponent choose random: improve robustness\n",
        "\n",
        "            if self.is_switch(my_action):\n",
        "                num_switches += 1\n",
        "\n",
        "            step_result = state.step([my_action, opp_action])\n",
        "            if isinstance(step_result, tuple):\n",
        "                next_states = step_result[0]\n",
        "                if next_states:\n",
        "                    state = next_states[0]\n",
        "            cycles += 1\n",
        "        return evaluate_terminal_state(state, cycles, num_switches)\n",
        "\n",
        "\n",
        "    def back_propagate(self, node, utility):\n",
        "        while node is not None:\n",
        "            node.visits += 1\n",
        "            node.utility += utility\n",
        "            node = node.parent\n",
        "\n",
        "    def ucb1(self, node):\n",
        "        if node.visits == 0:\n",
        "            return math.inf\n",
        "        return node.utility / node.visits + self.C * math.sqrt(math.log(node.parent.visits + 1) / node.visits)\n",
        "\n",
        "    def expand(self, node):\n",
        "        if node.is_fully_expanded():\n",
        "            return node.best_child()\n",
        "\n",
        "        for i in range(DEFAULT_N_ACTIONS):\n",
        "            if all(child.connection_action != i for child in node.children):\n",
        "                # skip this action if it's a suboptimal move\n",
        "                if is_action_suboptimal(node.game_state, i):\n",
        "                    continue\n",
        "\n",
        "                new_node = Node(node, i)\n",
        "                new_node.game_state = deepcopy(node.game_state)\n",
        "                opp_action = random.randint(0, DEFAULT_N_ACTIONS - 1)\n",
        "                step_result = new_node.game_state.step([i, opp_action])\n",
        "                if isinstance(step_result, tuple):\n",
        "                    next_states = step_result[0]\n",
        "                    if next_states:\n",
        "                        new_node.game_state = next_states[0]\n",
        "                node.children.append(new_node)\n",
        "                return new_node\n",
        "\n",
        "    def mcts_search(self, root):\n",
        "        for _ in (tqdm(range(self.max_iterations), desc='One turn simulations') if self.debug else range(self.max_iterations)):\n",
        "            current = root\n",
        "            # Selection\n",
        "            while current.children and current.is_fully_expanded():\n",
        "                current = max(current.children, key=self.ucb1)\n",
        "            # Expansion\n",
        "            if not current.is_fully_expanded():\n",
        "                current = self.expand(current)\n",
        "            # Simulation\n",
        "            utility = self.playout(current)\n",
        "            # Backpropagation\n",
        "            self.back_propagate(current, utility)\n",
        "\n",
        "        if len(root.children) == 0:\n",
        "            if self.debug:\n",
        "                print('No children, going random')\n",
        "            return random.randint(0, DEFAULT_N_ACTIONS - 1)\n",
        "\n",
        "        best_action = max(root.children, key=lambda c: c.utility / (c.visits + 1e-6), default=None)\n",
        "\n",
        "        if self.debug:\n",
        "            if best_action:\n",
        "                print(f\"Best action: {best_action.connection_action}, Reward: {best_action.utility}, Visits: {best_action.visits}\")\n",
        "            else:\n",
        "                print('No best action found, going random')\n",
        "\n",
        "        return best_action.connection_action if best_action else random.randint(0, DEFAULT_N_ACTIONS - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIDbeoZ1CodV"
      },
      "source": [
        "### *Selection*  \n",
        "\n",
        "The selection phase is performed using the **UCB1** formula:  \n",
        "\n",
        "$UCB1 (n) = \\frac{U(n)}{N(n)} + C \\cdot \\sqrt{\\frac{\\log N (\\text{Parent}(n))}{N(n)}}$  \n",
        "\n",
        "Where:  \n",
        "- $U(n)$ is the total utility of all playouts that passed through node $n$.  \n",
        "- $N(n)$ is the total number of playouts that passed through node $n$.  \n",
        "- $C$ is a balancing factor between the left term (_\"exploitation\"_ term) and the right term (_\"exploration\"_ term).  \n",
        "- $\\text{Parent}(n)$ is the parent node of $n$ in the tree.  \n",
        "\n",
        "Typically, $C$ is set to $\\sqrt{2}$, but as stated in the AIMA book, game developers often fine-tune this parameter.  \n",
        "\n",
        "This is precisely what I did! Through experimentation, I found that a suitable value for $C$ is $10$. Additionally, since the early stages of the game provide limited information about the opponent, $C$ changes dynamically over time.  \n",
        "\n",
        "In the initial turns, I prioritize _exploration_, aiming to identify a balanced move‚Äîone that is not necessarily optimized but generally effective. However, $C$ rapidly decreases to $10$, following the formula defined in the `get_action` method of the battle policy:  \n",
        "\n",
        "```python\n",
        "self.C = max(self.min_C, self.max_C / (self.turns ** 1.2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOItOkuTG0t2"
      },
      "source": [
        "### *Expansion*  \n",
        "\n",
        "From the root node and throughout the entire tree, we must decide how to expand nodes. The standard MCTS algorithm expands by considering all possible actions. This was the approach I initially implemented in the first version of the code. However, after realizing that this approach was not optimal, I transitioned to a **pruned** version.  \n",
        "\n",
        "Over successive development iterations, **pruning** evolved into a strategic component of the algorithm.  \n",
        "\n",
        "The goal is to avoid the expansion of **suboptimal actions**. In the code snippet below, the `is_action_suboptimal` function is used to prune ineffective actions based on factors such as the **opponent‚Äôs Pok√©mon types, attack stage, and weather conditions**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y2fyqCtSzEgR"
      },
      "outputs": [],
      "source": [
        "def n_fainted(t: PkmTeam): # from [@thunder battle policy]\n",
        "    fainted = 0\n",
        "    fainted += t.active.hp == 0\n",
        "    if len(t.party) > 0:\n",
        "        fainted += t.party[0].hp == 0\n",
        "    if len(t.party) > 1:\n",
        "        fainted += t.party[1].hp == 0\n",
        "    return fainted\n",
        "\n",
        "def estimate_damage(move: PkmMove, pkm_type: PkmType, opp_pkm_type: PkmType,\n",
        "                    attack_stage: int, defense_stage: int, weather: WeatherCondition) -> float: # from [@thunder battle policy]\n",
        "    move_type: PkmType = move.type\n",
        "    move_power: float = move.power\n",
        "    type_rate = TYPE_CHART_MULTIPLIER[move_type][opp_pkm_type]\n",
        "    if type_rate == 0:\n",
        "        return 0\n",
        "    if move.fixed_damage > 0:\n",
        "        return move.fixed_damage\n",
        "    stab = 1.5 if move_type == pkm_type else 1.\n",
        "    if (move_type == PkmType.WATER and weather == WeatherCondition.RAIN) or (\n",
        "            move_type == PkmType.FIRE and weather == WeatherCondition.SUNNY):\n",
        "        weather = 1.5\n",
        "    elif (move_type == PkmType.WATER and weather == WeatherCondition.SUNNY) or (\n",
        "            move_type == PkmType.FIRE and weather == WeatherCondition.RAIN):\n",
        "        weather = .5\n",
        "    else:\n",
        "        weather = 1.\n",
        "    stage_level = attack_stage - defense_stage\n",
        "    stage = (stage_level + 2.) / 2 if stage_level >= 0. else 2. / \\\n",
        "        (np.abs(stage_level) + 2.)\n",
        "    damage = type_rate * \\\n",
        "        stab * weather * stage * move_power\n",
        "    return damage\n",
        "\n",
        "def is_action_suboptimal(game_state: GameState, action: int, effectiveness_threshold: float = 0.5) -> bool:\n",
        "    active_moves = game_state.teams[0].active.moves\n",
        "\n",
        "    # check if the action is a switch\n",
        "    if action >= len(active_moves):\n",
        "        switch_index = action - len(active_moves)\n",
        "        my_team = game_state.teams[0]\n",
        "\n",
        "        if switch_index >= len(my_team.party) or my_team.party[switch_index].hp <= 0:\n",
        "            return True  # avoid switching to a fainted Pok√©mon\n",
        "\n",
        "        # let's eval the matchup of the current pokemon and of the switch pokemon\n",
        "        current_matchup = evaluate_actives_matchup(\n",
        "            my_team.active,\n",
        "            game_state.teams[1].active\n",
        "        )\n",
        "\n",
        "        switch_matchup = evaluate_actives_matchup(\n",
        "            my_team.party[switch_index],\n",
        "            game_state.teams[1].active\n",
        "        )\n",
        "\n",
        "        # if the switch doesn't improve the matchup by a threshold, consider it suboptimal\n",
        "        is_switch_suboptimal = (switch_matchup - SWITCH_THRESHOLD) < current_matchup\n",
        "        return is_switch_suboptimal\n",
        "\n",
        "    # for the attack moves let's compute the expected damage with `estimate_damage()`\n",
        "    move = active_moves[action]\n",
        "    attacker = game_state.teams[0].active\n",
        "    defender = game_state.teams[1].active\n",
        "\n",
        "    my_team = game_state.teams[0]\n",
        "    my_attack_stage = my_team.stage[PkmStat.ATTACK]\n",
        "\n",
        "    opp_team = game_state.teams[1]\n",
        "    opp_defense_stage = opp_team.stage[PkmStat.DEFENSE]\n",
        "\n",
        "    estimated_dmg = estimate_damage(\n",
        "        move=move,\n",
        "        pkm_type=attacker.type,\n",
        "        opp_pkm_type=defender.type,\n",
        "        attack_stage=my_attack_stage,\n",
        "        defense_stage=opp_defense_stage,\n",
        "        weather=game_state.weather\n",
        "    )\n",
        "\n",
        "    # if the damage is too low, i'll consider the move suboptimal\n",
        "    defender_max_hp = defender.max_hp\n",
        "    damage_ratio = estimated_dmg / defender_max_hp  # percentage of removed hp\n",
        "\n",
        "    is_suboptimal = damage_ratio < DAMAGE_THRESHOLD or estimated_dmg <= effectiveness_threshold\n",
        "    return is_suboptimal\n",
        "\n",
        "def evaluate_actives_matchup(my_active: Pkm, opp_active:Pkm) -> float:\n",
        "\n",
        "    my_active_type = my_active.type\n",
        "    opp_active_type = opp_active.type\n",
        "    my_moves_type = [m.type for m in my_active.moves]\n",
        "    opp_moves_type = [m.type for m in opp_active.moves if m.name]\n",
        "\n",
        "    # calculate base defensive matchup considering STAB\n",
        "    defensive_match_up = 0.\n",
        "    for mtype in opp_moves_type:\n",
        "        multiplier = TYPE_CHART_MULTIPLIER[mtype][my_active_type]\n",
        "        # apply STAB bonus if move type matches attacker's type\n",
        "        if mtype == opp_active_type:\n",
        "            multiplier *= 1.5\n",
        "        defensive_match_up = max(multiplier, defensive_match_up)\n",
        "\n",
        "    # calculate base offensive matchup considering STAB\n",
        "    offensive_match_up = 0.\n",
        "    for mtype in my_moves_type:\n",
        "        multiplier = TYPE_CHART_MULTIPLIER[mtype][my_active_type]\n",
        "        # apply STAB bonus if move type matches attacker's type\n",
        "        if mtype == opp_active_type:\n",
        "            multiplier *= 1.5\n",
        "        offensive_match_up = max(multiplier, offensive_match_up)\n",
        "\n",
        "    # calculate type coverage scores\n",
        "    my_coverage_score = 0\n",
        "    opp_coverage_score = 0\n",
        "\n",
        "    # evaluate how many different types we can hit effectively\n",
        "    type_coverage = set()\n",
        "    for mtype in my_moves_type:\n",
        "        for target_type in PkmType:\n",
        "            if TYPE_CHART_MULTIPLIER[mtype][target_type] > 1:\n",
        "                type_coverage.add(target_type)\n",
        "    my_coverage_score = len(type_coverage) / len(PkmType)\n",
        "\n",
        "    # evaluate opponent's type coverage\n",
        "    opp_type_coverage = set()\n",
        "    for mtype in opp_moves_type:\n",
        "        for target_type in PkmType:\n",
        "            if TYPE_CHART_MULTIPLIER[mtype][target_type] > 1:\n",
        "                opp_type_coverage.add(target_type)\n",
        "    opp_coverage_score = len(opp_type_coverage) / len(PkmType)\n",
        "\n",
        "    # calculate immunities and resistances\n",
        "    immunity_bonus = 0\n",
        "    for mtype in opp_moves_type:\n",
        "        if TYPE_CHART_MULTIPLIER[mtype][my_active_type] == 0:\n",
        "            immunity_bonus += 0.5\n",
        "        elif TYPE_CHART_MULTIPLIER[mtype][my_active_type] <= 0.5:\n",
        "            immunity_bonus += 0.25\n",
        "\n",
        "    # combine all factors\n",
        "    matchup_score = (offensive_match_up * 1.2  # Weighted more towards offense üò°\n",
        "                    - defensive_match_up\n",
        "                    + my_coverage_score * 0.5\n",
        "                    - opp_coverage_score * 0.3\n",
        "                    + immunity_bonus)\n",
        "\n",
        "    return matchup_score\n",
        "\n",
        "# statis condition evaluation with precise weights\n",
        "def detailed_status_eval(pkm: Pkm) -> float:\n",
        "    if pkm.status == PkmStatus.PARALYZED:\n",
        "        return -1.2  # severe speed penalty and chance to not move\n",
        "    elif pkm.status == PkmStatus.SLEEP:\n",
        "        return -1.5  # cannot move but temporary\n",
        "    elif pkm.status == PkmStatus.FROZEN:\n",
        "        return -1.8  # cannot move and rare to thaw\n",
        "    elif pkm.status == PkmStatus.CONFUSED:\n",
        "        return -0.8  # temporary but dangerous\n",
        "    elif pkm.status == PkmStatus.BURNED:\n",
        "        return -0.9  # attack reduction and damage\n",
        "    elif pkm.status == PkmStatus.POISONED:\n",
        "        return -0.7  # just damage\n",
        "    return 0\n",
        "\n",
        "def evaluate_stages(team: PkmTeam) -> float:\n",
        "\n",
        "    stage_score = 0\n",
        "\n",
        "    def update_stage_score(stage: any, weight: float) -> float:\n",
        "        if stage > 0:\n",
        "            return weight * (1 - (0.9 ** stage))\n",
        "        else:\n",
        "            return -weight * (1 - (0.9 ** abs(stage)))\n",
        "\n",
        "    attack_stage = team.stage[PkmStat.ATTACK]\n",
        "    defense_stage = team.stage[PkmStat.DEFENSE]\n",
        "    speed_stage = team.stage[PkmStat.SPEED]\n",
        "\n",
        "    stage_score += update_stage_score(attack_stage, 1.2)\n",
        "    stage_score += update_stage_score(defense_stage, 1.0)\n",
        "    stage_score += update_stage_score(speed_stage, 1.1)\n",
        "\n",
        "    return stage_score\n",
        "\n",
        "def evaluate_team_matchups(game_state: GameState) -> List[float]:\n",
        "    my_team = game_state.teams[0]\n",
        "    opp_active = game_state.teams[1].active\n",
        "    weather = game_state.weather\n",
        "    matchup_scores = []\n",
        "\n",
        "    for pokemon in [my_team.active] + my_team.party:\n",
        "        if pokemon.hp <= 0:  # Skip fainted Pok√©mon\n",
        "            matchup_scores.append(float('-inf'))\n",
        "            continue\n",
        "\n",
        "        matchup = evaluate_actives_matchup(\n",
        "            pokemon,\n",
        "            opp_active\n",
        "        )\n",
        "\n",
        "        # consider weather benefits for this Pok√©mon\n",
        "        if (weather == WeatherCondition.SUNNY and pokemon.type == PkmType.FIRE) or \\\n",
        "           (weather == WeatherCondition.RAIN and pokemon.type == PkmType.WATER):\n",
        "            matchup += 0.5  # boost matchup score\n",
        "\n",
        "        if (weather == WeatherCondition.SANDSTORM and pokemon.type == PkmType.ROCK):\n",
        "            matchup += 0.3  # rock-types get Sp. Def boost\n",
        "\n",
        "        if (weather == WeatherCondition.HAIL and pokemon.type == PkmType.ICE):\n",
        "            matchup += 0.2  # ice-types are immune to Hail\n",
        "\n",
        "        matchup_scores.append(matchup)\n",
        "\n",
        "    return matchup_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qarVmjjL3nk"
      },
      "source": [
        "### *Simulation (Playout)*  \n",
        "\n",
        "Now that we have selected and expanded a non-suboptimal node, we must simulate the entire game until reaching a terminal state.  \n",
        "\n",
        "#### *Playout Policy*  \n",
        "\n",
        "To guide the simulation, we need to define a **playout policy**, which determines how actions are chosen sequentially.  \n",
        "\n",
        "The traditional playout strategy follows a **purely random** approach.  \n",
        "\n",
        "Each transition between game states can only be triggered if both our battle policy and the opponent's policy provide an action.  \n",
        "\n",
        "The playout policy in my implementation follows a strategy where it **creates a pool of non-suboptimal moves** and then **selects one randomly**, leveraging the intrinsic robustness of MCTS.  \n",
        "\n",
        "*What about the opponent?*\n",
        "\n",
        "In an earlier versions of the code, I implemented a strategy where the **opponent followed the same selection process** as our policy. However, this approach had limitations due to the lack of information available about the opponent.  \n",
        "\n",
        "Moreover, empirical testing showed poor results with this method.  \n",
        "\n",
        "In the latest version, the opponent plays **purely random** moves.  \n",
        "\n",
        "#### *Number of Simulations*  \n",
        "\n",
        "The number of simulations was tuned to achieve a competitive MCTS based battle policy. After extensive testing, I found that an optimal number of simulations is:  \n",
        "\n",
        "$\\textbf{O(10^5)}$  \n",
        "\n",
        "Each search can take anywhere from one to 10 minutes, depending on the pruning of suboptimal actions. Since the framework does not specify any time constraints, I considered a 10-minute runtime acceptable.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9VmwrA3Pkst"
      },
      "source": [
        "### *Backpropagation and Utility*  \n",
        "\n",
        "When the simulation reaches a terminal state, a **utility function** evaluates whether the outcome was favorable.  \n",
        "\n",
        "A simple scoring system, such as assigning 1 for a win and -1 for a loss, would be an oversimplification.  \n",
        "\n",
        "To improve this evaluation, I incorporated additional factors, including:  \n",
        "- **Matchup score**  \n",
        "- **Remaining health points**  \n",
        "- **Pok√©mon status conditions** (e.g., poisoned, paralyzed, etc.)  \n",
        "- **And more...** (Refer to the `evaluate_terminal_state()` function for a complete list.)  \n",
        "\n",
        "These factors are combined into a weighted sum.  \n",
        "\n",
        "Once the utility is calculated, the **results are backpropagated** through the tree up to the root node.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SfW-m4Icz2N2"
      },
      "outputs": [],
      "source": [
        "def evaluate_terminal_state(state: GameState, cycles: int, num_switches: int) -> float:\n",
        "    my_team = state.teams[0]\n",
        "    opp_team = state.teams[1]\n",
        "    my_active: Pkm = my_team.active\n",
        "    opp_active: Pkm = opp_team.active\n",
        "\n",
        "    # base matchup evaluation\n",
        "    match_up = evaluate_actives_matchup(\n",
        "        my_active,\n",
        "        opp_active\n",
        "    )\n",
        "\n",
        "    # hp evaluation\n",
        "    my_hp_ratio = my_active.hp / my_active.max_hp\n",
        "    opp_hp_ratio = opp_active.hp / opp_active.max_hp\n",
        "\n",
        "    # hp is more valuable when it's lower\n",
        "    hp_weight = 4.0\n",
        "    my_hp_score = hp_weight * (1 + (1 - my_hp_ratio)) * my_hp_ratio\n",
        "    opp_hp_score = hp_weight * (1 + (1 - opp_hp_ratio)) * opp_hp_ratio\n",
        "\n",
        "    # team health evaluation\n",
        "    my_team_health = 0\n",
        "    for pokemon in [my_team.party[0], my_team.party[1]]:\n",
        "        if pokemon.hp > 0:\n",
        "            health_ratio = pokemon.hp / pokemon.max_hp\n",
        "            # backup pokemon health is important\n",
        "            my_team_health += health_ratio * 2\n",
        "\n",
        "    my_status_score = detailed_status_eval(my_active)\n",
        "    opp_status_score = detailed_status_eval(opp_active)\n",
        "\n",
        "    my_stage_score = evaluate_stages(my_team)\n",
        "    opp_stage_score = evaluate_stages(opp_team)\n",
        "\n",
        "\n",
        "    # weather\n",
        "    weather_score = 0\n",
        "    if state.weather != WeatherCondition.CLEAR:\n",
        "        if (state.weather == WeatherCondition.SUNNY and my_active.type == PkmType.FIRE) or \\\n",
        "           (state.weather == WeatherCondition.RAIN and my_active.type == PkmType.WATER):\n",
        "            weather_score += 0.5\n",
        "        elif (state.weather == WeatherCondition.SUNNY and opp_active.type == PkmType.FIRE) or \\\n",
        "             (state.weather == WeatherCondition.RAIN and opp_active.type == PkmType.WATER):\n",
        "            weather_score -= 0.5\n",
        "\n",
        "    # switching penalty\n",
        "    switch_penalty = -0.15 * num_switches * max(0, (30 - cycles) / 30)\n",
        "\n",
        "    utility = (\n",
        "        match_up * 1.2 +                    # matchup\n",
        "        my_hp_score - opp_hp_score +        # active pkm hp\n",
        "        my_team_health * 0.8 +              # team health\n",
        "        my_status_score - opp_status_score + # status conditions\n",
        "        my_stage_score * 0.3 -              # stat stages\n",
        "        opp_stage_score * 0.3 +             # opp stat stages\n",
        "        weather_score +                      # weather effects\n",
        "        switch_penalty                       # switch penalty\n",
        "    )\n",
        "\n",
        "    return utility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xOLg4MYSWEr"
      },
      "source": [
        "## **Assessment**  \n",
        "\n",
        "The evaluation of the battle policy was conducted against four different pre-built policies, arranged in increasing order of complexity:  \n",
        "1. `TypeSelector` policy  \n",
        "2. `PrunedBFS` policy  \n",
        "3. `Minimax` policy  \n",
        "4. `TunedTreeSearch` policy  \n",
        "\n",
        "Although the original competition rules state that each match should consist of 10 battles with a team switch halfway through, I opted for three tests.\n",
        "\n",
        "Each test consisted of two games, with teams being swapped at the end of each game to eliminate potential team imbalance.  \n",
        "\n",
        "Each game comprised two or three battles, depending on the match outcomes.\n",
        "\n",
        "Here's the Results (rounded to the nearest integer):  \n",
        "\n",
        "| Opponent Policy      | MCTS Win Rate | Avg. No. of Turns | Total Battles |\n",
        "|--------------------|---------|-------------------|--------------|\n",
        "| `TypeSelector`   | 79%  | 7             | 14       |\n",
        "| `PrunedBFS`      | 64%  | 6             | 14       |\n",
        "| `Minimax`        | 100%  | 8             | 12        |\n",
        "| `TunedTreeSearch`| 64%  | 8             |14       |\n",
        "\n",
        "\n",
        "- Against `Minimax` the MCTS achieved an **100% win rate**, suggesting that our policy effectively countered its decision-making process despite longer battles (8 turns on average).  \n",
        "- Against the `TypeSelector` policy, the **79% win rate** indicates that the pruning strategy was effective in exploiting type-based advantages.  \n",
        "- `PrunedBFS` and the  `TunedTreeSearch` policies posed more of a challenge (**64% win rate**).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GVO1TAFz6RN"
      },
      "outputs": [],
      "source": [
        "competitor0 = CompetitorWrapper('Player0')\n",
        "competitor1 = CompetitorWrapper('Player1')\n",
        "\n",
        "competitor0._battle_policy = MCTSPolicy(max_iterations=10_000, debug=True)\n",
        "competitor1._battle_policy = Minimax() # choose here the opponent policy: Minimax, TypeSelector, TunedTreeTraversal, PrunedBFS\n",
        "\n",
        "cm0 = CompetitorManager(competitor0)\n",
        "cm1 = CompetitorManager(competitor1)\n",
        "\n",
        "roster = RandomPkmRosterGenerator().gen_roster()\n",
        "team0 = RandomTeamFromRoster(roster).get_team()\n",
        "team1 = RandomTeamFromRoster(roster).get_team()\n",
        "\n",
        "cm0.team = team0\n",
        "cm1.team = team1\n",
        "\n",
        "match = BattleMatch(cm0, cm1, debug=True)\n",
        "match.run()\n",
        "\n",
        "print(f'You...{\"WON! üéâ\" if match.winner() == 0 else \"loose...üò®\"}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssLuMTXlXJqI"
      },
      "source": [
        "## **Conclusion**  \n",
        "\n",
        "Working on this project was a deeply rewarding experience. The Monte Carlo Tree Search method demonstrated its strength as a decision-making algorithm in Pok√©mon battles. Integrating heuristics significantly boosted its effectiveness by guiding MCTS to prune suboptimal moves and focus on more promising strategies.  \n",
        "\n",
        "This refinement not only improved efficiency but also reduced unnecessary or ineffective actions.  \n",
        "\n",
        "Additionally, the inherent randomness in MCTS contributed to the robustness of the policy, enabling it to adapt to the stochastic nature of Pok√©mon battles.  \n",
        "\n",
        "By combining strategic pruning through heuristics with randomized exploration, the approach achieved a well-balanced capability to handle uncertainty and imperfect information.  \n",
        "\n",
        "This project greatly enhanced my understanding of game AI, decision-making under uncertainty, and heuristic-driven search methods. There is still substantial room for improvement, and I am particularly interested to explore the integration of reinforcement learning, as its potential deeply intrigues me.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZvO4kCoZc8e"
      },
      "source": [
        "## **Citations**  \n",
        "\n",
        "[1] Reis, S., et al. (2021). *VGC AI Competition - A New Model of Meta-Game Balance AI Competition* [IEEE Conference Of Games](https://ieee-cog.org/2021/assets/papers/paper_6.pdf) | *Pok√©mon VGC Engine Repository* [GitLab](https://gitlab.com/DracoStriker/pokemon-vgc-engine).\n",
        "\n",
        "[2] Norstr√∂m, J. (2019). *Comparison of Artificial Intelligence Algorithms for Pok√©mon Battles*. [Chalmers University of Technology](https://odr.chalmers.se/server/api/core/bitstreams/b5fac289-1328-41b8-ad78-f89eb39fce30/content)  \n",
        "\n",
        "[3] Ihara H., et al. (2018). *Implementation and Evaluation of Information Set Monte Carlo Tree Search for Pok√©mon*. [IEEE Xplore](https://ieeexplore.ieee.org/document/8616371/)  \n",
        "\n",
        "[4] Kalose, K., & Kaya, M. (2018). *Optimal Battle Strategy in Pok√©mon using Reinforcement Learning*. [Semantic Scholar](https://www.semanticscholar.org/paper/Optimal-Battle-Strategy-in-Pok%C3%A9mon-using-Learning-Kalose-Kaya/897f281adb99d158c2f53fb68d0e20ea510a7cab)  \n",
        "\n",
        "[5] Rodriguez, G., et al. (2024). *Enhancing Pok√©mon VGC Player Performance: Intelligent Agents Through Deep Reinforcement Learning and Neuroevolution*. [ACM Digital Library](https://dl.acm.org/doi/10.1007/978-3-031-60692-2_19)  \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
